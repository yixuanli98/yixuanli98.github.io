<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Yixuan Li</title>

    <meta name="author" content="Yixuan Li">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Yixuan Li (ÊùéÂ•ïËê±)
                </p>
                <p>I am currently a fourth-year Ph.D. student at <a href="http://mmlab.ie.cuhk.edu.hk/index.html">MMLab</a> in <a href="https://www.ie.cuhk.edu.hk/main/index.shtml">Department of Information Engineering</a>, <a href="https://www.cuhk.edu.hk/english/index.html">CUHK</a>, advised by <a href="http://dahua.site/">Prof. Dahua Lin</a>. 
                Before that, I received my Master's degree from Nanjing University in 2022, supervised by <a href="http://wanglimin.github.io/">Prof. Limin Wang</a>, and my Bachelor's degree also from Nanjing University in 2019. 
                  
                  My research area is 3D vision, especially 3D Scene Reconstruction and Generation, and video generation.
                <p style="text-align:center">
                  <a href="mailto:liyixxxuan@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=dC3bpFcAAAAJ">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://twitter.com/liyixxxuan">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://github.com/yixuanli98">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/liyixuan.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/liyixuan.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <h2>News</h2>
            <p>
              &bull; 06/2025 Two papers accepted by ICCV 2025. <br>
              &bull; 04/2025 One paper accepted by CVPR Workshop 2025. <br>
              &bull; 09/2024 Two papers accepted by NeurIPS 2024. <br>
              &bull; 02/2024 One paper accepted by CVPR 2024. <br>
              &bull; 02/2024 One paper accepted by TIP. <br>
              &bull; 06/2023 One paper accepted by ICCV 2023. <br>
              &bull; 10/2021 I got the National Scholarship. <br>
              &bull; 07/2021 One paper accepted by ICCV 2021. <br>
              &bull; 06/2021 We got the first place in the HC-STVG track of the CVPR 2021 workshop <a href="http://www.picdataset.com/">Person in Context</a>. <br>
              &bull; 04/2021 I was a student co-organizer of ICCV 2021 Workshop <a href="https://deeperaction.github.io/">DeeperAction</a>. <br>
              &bull; 10/2020 I got the National Scholarship. <br>
              &bull; 06/2020 One paper accepted by ECCV 2020. <br>
            </p>
          </td>
        </tr>
      </tbody></table>
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
              </td>
          </tbody></table>

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='multihuman_image'><video  width=100% height=100% muted autoplay loop>
            <source src="images/multihuman-video.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
        </td>

        <td style="padding:20px;width:75%;vertical-align:middle">
          <strong>Multi-identity Human Image Animation with Structural Video Diffusion</strong>
          <br>
          
          <a href="https://zhenzhiwang.github.io/">Zhenzhi Wang</a>, <strong>Yixuan Li</strong>, <a href="https://zengyh1900.github.io/">Yanhong Zeng</a >, <a href="https://guoyww.github.io/">Yuwei Guo</a >, <a href="http://dahua.site/">Dahua Lin</a >, <a href="https://tianfan.info/">Tianfan Xue</a >, <a href="https://daibo.info/">Bo Dai</a >.
          <br>
          <em>ICCV</em>, 2025
          <br>
          <a href="https://arxiv.org/abs/2504.04126">arXiv</a >
          <p></p >
          <p>We use identity-specific embeddings and structural learning with depth/surface-normal cues to handle complex multi-person interactions in human-centric video generation from a single image. We also contribute a dataset expansion with 25K multi-human interaction videos.</p >
        </td>
      </tr>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='direct3d_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/direct3d-video.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
            </td>
    
            <td style="padding:20px;width:75%;vertical-align:middle">
              <strong>Direct Numerical Layout Generation for 3D Indoor Scene Synthesis via Spatial Reasoning</strong>
              <br>
              
              <a href="https://scholar.google.com/citations?user=8AubXI4AAAAJ&hl=zh-CN">Xingjian Ran</a>, <strong>Yixuan Li</strong>, <a href="https://eveneveno.github.io/lnxu/">Linning Xu</a>,<a href="https://mulinyu.github.io/">Mulin Yu</a>
              <a href="https://daibo.info/">Bo Dai</a>.
              <br>
              <em>Arixv</em>
              <br>
              <a href="https://arxiv.org/abs/2506.05341">paper</a> /
              <a href="https://directlayout.github.io/">project page</a>
              <p></p>
              <p>We introduce DirectLayout, a framework that directly generates numerical 3D layouts from text descriptions, without relying on intermediate representations and constrained optimization. The model employs Chain-of-Thought reasoning and design CoT-Grounded Generative Layout Reward to enhance spatial planning and generalization. Extensive experiments demonstrate that DirectLayout achieves impressive semantic consistency, generalization and physical plausibility.</p>
            </td>
          </tr>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='db3d_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/building_editing.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
            </td>
    
            <td style="padding:20px;width:75%;vertical-align:middle">
              <strong>Proc-GS: Procedural Building Generation for City Assembly with 3D Gaussians</strong>
              <br>
              
              <strong>Yixuan Li</strong>, <a href="https://scholar.google.com/citations?user=8AubXI4AAAAJ&hl=zh-CN">Xingjian Ran</a>, <a href="https://eveneveno.github.io/lnxu/">Linning Xu</a>,<a href="https://inspirelt.github.io/">Tao Lu</a>, <a href="https://mulinyu.github.io/">Mulin Yu</a>
              <a href="https://zhenzhiwang.github.io/">Zhenzhi Wang</a>, <a href="https://kam1107.github.io/">Yuanbo Xiangli</a>, <a href="http://dahua.site/">Dahua Lin</a>, <a href="https://daibo.info/">Bo Dai</a>.
              <br>
              <em>CVPR Workshop on Urban Scene Modeling</em>, 2025, Spotlight
              <br>
              <a href="https://arxiv.org/abs/2412.07660">paper</a> /
              <a href="https://city-super.github.io/procgs/">project page</a> /
              <a href="https://github.com/city-super/ProcGS/">code</a>
              <p></p>
              <p>Proc-GS combines procedural modeling with 3D Gaussian Splatting, enabling efficient generation of diverse buildings with high rendering quality. This integration allows scalable city creation with precise control for both real and synthetic scenarios. </p>
            </td>
          </tr>
      
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='humanvid_image'><video  width=100% height=100% muted autoplay loop>
            <source src="images/matrix_w_legend.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
        </td>

        <td style="padding:20px;width:75%;vertical-align:middle">
          <strong>HumanVid: Demystifying Training Data for Camera-controllable Human Image Animation</strong>
          <br>
          
          <a href="https://zhenzhiwang.github.io/">Zhenzhi Wang</a>, <strong>Yixuan Li</strong>, <a href="https://zengyh1900.github.io/">Yanhong Zeng</a >, <a href="#">Youqing Fang</a >, <a href="https://guoyww.github.io/">Yuwei Guo</a >, <a href="#">Wenran Liu</a >, <a href="https://sparkstj.github.io/">Jing Tan</a >, <a href="https://chenkai.site/">Kai Chen</a >, <a href="https://tianfan.info/">Tianfan Xue</a >, <a href="https://daibo.info/">Bo Dai</a >, <a href="http://dahua.site/">Dahua Lin</a >.
          <br>
          <em>NeurIPS (Datasets and Benchmarks Track)</em>, 2024
          <br>
          <a href="https://arxiv.org/abs/2407.17438">arXiv</a > /
          <a href="https://humanvid.github.io">homepage</a > /
          <a href="https://github.com/zhenzhiwang/HumanVid">code</a >
          <p></p >
          <p>We propose camera-controllable human image animation task for generating video clips that are similar to real movie clips. To achieve this, we collect a dataset named HumanVid, and a baseline model combined by Animate Anyone and CameraCtrl. Without any tricks, we show that a simple baseline trained on our dataset could generate movie-level video clips.</p >
        </td>
      </tr>

<!--           <tr onmouseout="layerpano3d_stop()" onmouseover="layerpano3d_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='layerpano3d_image'><video  width=100% muted autoplay loop>
          <source src="images/layerpano3d.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/layerpano3d.png' width=100%>
        </div>
        <script type="text/javascript">
          function layerpano3d_start() {
            document.getElementById('layerpano3d_image').style.opacity = "1";
          }

          function layerpano3d_stop() {
            document.getElementById('layerpano3d_image').style.opacity = "0";
          }
          layerpano3d_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <strong>LayerPano3D: Layered 3D Panorama for Hyper-Immersive Scene Generation</strong>
        <br>
        <a href="https://ys-imtech.github.io/">Shuai Yang*</a>,
        <a href="ttps://sparkstj.github.io/">Jing Tan*</a>,
        <a href="https://github.com/kszpxxzmc/">Mengchen Zhang</a>,
        <a href="https://wutong16.github.io/">Tong Wu</a>,
        <strong>Yixuan Li</strong>,
        <a href="https://stanford.edu/~gordonwz/">Gordon Wetzstein</a>,
        <a href="https://liuziwei7.github.io/">Ziwei Liu</a>,
        <a href="http://dahua.site/">Dahua Lin</a>
        <br>
        <em>Arxiv</em>, 2024
        <br>
        <a href="https://ys-imtech.github.io/projects/LayerPano3D/">project page</a>
        /
        <a href="https://www.youtube.com/watch?v=dXvoFRbHOiw">video</a>
        /
        <a href="https://arxiv.org/abs/2408.13252">arXiv</a>
        <p></p>
        <p>
        LayerPano3D generates full-view, explorable panoramic 3D scene from a single text prompt.
        </p>
      </td>
    </tr> -->

<!--       <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='intercontrol_image'><video  width=100% height=100% muted autoplay loop>
            <source src="images/intercontrol.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
        </td>

        <td style="padding:20px;width:75%;vertical-align:middle">
          <strong>InterControl: Generate Human Motion Interactions by Controlling Every Joint</strong>
          <br>
          
          <a href="https://zhenzhiwang.github.io/">Zhenzhi Wang</a>, <a href="https://scholar.google.co.uk/citations?user=GStTsxAAAAAJ&hl=zh-CN">Jingbo Wang</a>, <strong>Yixuan Li</strong>, <a href="http://dahua.site/">Dahua Lin</a>, <a href="https://daibo.info/">Bo Dai</a>.
          <br>
          <em>NeurIPS</em>, 2024
          <br>
          <a href="https://arxiv.org/abs/2311.15864">arXiv</a> /
          <a href="https://github.com/zhenzhiwang/intercontrol">code</a>
          <p></p>
          <p>We could generate human motion interactions with spatially controllable MDM that is only trained on single-person data.</p>
        </td>
      </tr> -->

<!--           <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='pacer+_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/pacer+.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
            </td>
    
            <td style="padding:20px;width:75%;vertical-align:middle">
              <strong>PACER+: On-Demand Pedestrian Animation Controller in Driving Scenarios</strong>
              <br>
              
              <a href="https://scholar.google.co.uk/citations?user=GStTsxAAAAAJ">Jingbo Wang</a>, <a href="https://www.zhengyiluo.com/">Zhengyi Luo</a>, <a href="https://ye-yuan.com/">Ye Yuan</a>, <strong>Yixuan Li</strong>, <a href="https://daibo.info/">Bo Dai</a>.
              <br>
              <em>CVPR</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2404.19722">arXiv</a> /
              <a href="https://github.com/IDC-Flash/PacerPlus">code</a>
              <p></p>
              <p>We could generate human motion interactions with spatially controllable MDM that is only trained on single-person data.</p>
            </td>
          </tr> -->
          

       <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='db3d_image'><video  width=100% height=100% muted autoplay loop>
            <source src="images/matrixcity.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
        </td>

        <td style="padding:20px;width:75%;vertical-align:middle">
          <strong>MatrixCity: A Large-scale City Dataset for City-scale Neural Rendering and Beyond</strong>
          <br>
          
          <strong>Yixuan Li</strong>*, <a href="https://jianglh-whu.github.io/">Lihan Jiang</a>*, <a href="https://eveneveno.github.io/lnxu/">Linning Xu</a>,<a href="https://kam1107.github.io/">Yuanbo Xiangli</a>, 
          <a href="https://zhenzhiwang.github.io/">Zhenzhi Wang</a>, <a href="http://dahua.site/">Dahua Lin</a>, <a href="https://daibo.info/">Bo Dai</a>.
          <br>
          <em>ICCV</em>, 2023
          <br>
          <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Li_MatrixCity_A_Large-scale_City_Dataset_for_City-scale_Neural_Rendering_and_ICCV_2023_paper.pdf">paper</a> /
          <a href="https://city-super.github.io/matrixcity/">project page</a> /
          <a href="https://github.com/city-super/MatrixCity">code</a>
          <p></p>
          <p>A large scale synthetic dataset from Unreal Engine 5 for city-scale NeRF rendering. </p>
        </td>
      </tr>


      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one"><div class="two" id='dmn_image'>
                    <img src='images/STDet_teaser.png' width="160"></div></div>
        </td>

        <td style="padding:20px;width:75%;vertical-align:middle">
          <strong>Sparse Action Tube Detection</strong>
          <br>
          
          <strong>Yixuan Li</strong>*, <a href="https://zhenzhiwang.github.io/">Zhenzhi Wang</a>*, <a href="https://scholar.google.com/citations?user=VTrRNN4AAAAJ">Zhifeng Li</a>, <a href="https://wanglimin.github.io/">Limin Wang</a>.
          <br>
          <em>TIP</em>, 2024
          <br>
          <a href="https://ieeexplore.ieee.org/abstract/document/10458961">paper</a>
          <p></p>
          <p>We present a simple end-to-end action tube detection method, which reduces the dense hand-crafted anchors, captures longer temporal information and explictly predicts the action boundary. </p>
        </td>
      </tr>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one"><div class="two" id='dmn_image'>
                    <img src='images/multisports.jpg' width="160"></div></div>
        </td>

        <td style="padding:20px;width:75%;vertical-align:middle">
          <strong>MultiSports: A Multi-Person Video Dataset of Spatio-Temporally Localized Sports Actions</strong>
          <br>
          
          <strong>Yixuan Li</strong>, Lei Chen, Runyu He, <a href="https://zhenzhiwang.github.io/">Zhenzhi Wang</a>, Gangshan Wu, <a href="https://wanglimin.github.io/">Limin Wang</a>.
          <br>
          <em>ICCV</em>, 2021
          <br>
          one track of ICCV2021, ECCV2022 Workshop <a href="https://deeperaction.github.io">DeeperAction</a>. 
          <br>
          <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Li_MultiSports_A_Multi-Person_Video_Dataset_of_Spatio-Temporally_Localized_Sports_Actions_ICCV_2021_paper.pdf">paper</a> /
          <a href="https://github.com/MCG-NJU/MultiSports">code</a>
          <p></p>
          <p>A fine-grained and large-scale spatial-temporal action detection dataset with 4 different sports, 66 action categories.</p>
        </td>
      </tr>


      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one"><div class="two" id='dmn_image'>
                    <img src='images/moc_pipeline.jpg' width="160"></div></div>
        </td>

        <td style="padding:20px;width:75%;vertical-align:middle">
          <strong>Actions as Moving Points</strong>
          <br>
          
          <strong>Yixuan Li</strong>*, Zixu Wang*, <a href="https://wanglimin.github.io/">Limin Wang</a>, Gangshan Wu.
          <br>
          <em>ECCV</em>, 2020
          <br>
          <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123610069.pdf">paper</a> /
          <a href="https://github.com/MCG-NJU/MOC-Detector">code</a>
          <p></p>
          <p>A conceptually simple, computationally efficient, and more precise anchor-free action tubelet detector.</p>
        </td>
      </tr>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Professional Services</h2>
                <p>
                  &bull; Conference reviewer for CVPR, ICCV, ECCV, NeurIPS. <br>
                  &bull; Journal reviewer for IJCV, Pattern Recognition, TCSVT, Neurocomputing.<br>
                  &bull; Co-organizer of <a href="https://deeperaction.github.io/iccv21/index.html">DeeperAction Workshop</a> at ICCV 2021 and ECCV 2022.<br>
                </p>
              </td>
            </tr>
          </tbody></table>

          <br>
          <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=200&t=tt&d=m5r3KKO_lwqmT3gbTw8b7WeZN4e-OMtn1y05AxT7GBM&co=a1c4dd&cmo=f97f7f&cmn=6ba26b'></script>
          <br>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:center;font-size:small;">
                  Thanks <a href="https://jonbarron.info/">Jon Barron </a> for sharing the source code of this <a href="https://github.com/jonbarron/jonbarron_website">website template</a>.
                </p>
              </td>
            </tr>
          </tbody></table>

        </td>
      </tr>
    </table>
  </body>
</html>
