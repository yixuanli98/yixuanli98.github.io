<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Yixuan Li</title>

    <meta name="author" content="Yixuan Li">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Yixuan Li (ÊùéÂ•ïËê±)
                </p>
                <p>I am currently a second-year Ph.D. student at <a href="http://mmlab.ie.cuhk.edu.hk/index.html">MMLab</a> in <a href="https://www.ie.cuhk.edu.hk/main/index.shtml">Department of Information Engineering</a>, <a href="https://www.cuhk.edu.hk/english/index.html">CUHK</a>, advised by <a href="http://dahua.site/">Prof. Dahua Lin</a>. 
                Before that, I received my Master's degree from Nanjing University in 2022, supervised by <a href="http://wanglimin.github.io/">Prof. Limin Wang</a>, and my Bachelor's degree also from Nanjing University in 2019. 
                </p>
                <p>
                My research area is computer vision, especially <strong>3D scene reconstruction and generation</strong>, with a recent focus on neural rendering for large-scale scenes. My previous research interest was video understanding. I am open to discussions and collaborations about research, please feel free to send me emails.
                </p>
                <p style="text-align:center">
                  <a href="mailto:liyixxxuan@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=dC3bpFcAAAAJ">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://twitter.com/liyixxxuan">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://github.com/yixuanli98">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/liyixuan.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/liyixuan.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>News</h2>
                <p>
                  &bull; 02/2024 One paper accepted by CVPR 2024. <br>
                  &bull; 02/2024 One paper accepted by TIP. <br>
                  &bull; 06/2023 One paper accepted by ICCV 2023. <br>
                  &bull; 10/2021 I got the National Scholarship. <br>
                  &bull; 07/2021 One paper accepted by ICCV 2021. <br>
                  &bull; 06/2021 We got the first place in the HC-STVG track of the CVPR 2021 workshop <a href="http://www.picdataset.com/">Person in Context</a>. <br>
                  &bull; 04/2021 I was a student co-organizer of ICCV 2021 Workshop <a href="https://deeperaction.github.io/">DeeperAction</a>. <br>
                  &bull; 10/2020 I got the National Scholarship. <br>
                  &bull; 06/2020 One paper accepted by ECCV 2020. <br>
                </p>
              </td>
            </tr>
          </tbody></table>
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
              </td>
          </tbody></table>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='intercontrol_image'><video  width=100% height=100% muted autoplay loop>
            <source src="images/intercontrol.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
        </td>

        <td style="padding:20px;width:75%;vertical-align:middle">
          InterControl: Generate Human Motion Interactions by Controlling Every Joint
          <br>
          
          <a href="https://zhenzhiwang.github.io/">Zhenzhi Wang</a>, <a href="https://scholar.google.co.uk/citations?user=GStTsxAAAAAJ&hl=zh-CN">Jingbo Wang</a>, <strong>Yixuan Li</strong>, <a href="http://dahua.site/">Dahua Lin</a>, <a href="https://daibo.info/">Bo Dai</a>.
          <br>
          <em>Arxiv</em>, 2023
          <br>
          <a href="https://arxiv.org/abs/2311.15864">arXiv</a> /
          <a href="https://github.com/zhenzhiwang/intercontrol">code</a>
          <p></p>
          <p>We could generate human motion interactions with spatially controllable MDM that is only trained on single-person data.</p>
        </td>
      </tr>

       <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='db3d_image'><video  width=100% height=100% muted autoplay loop>
            <source src="images/matrixcity.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
        </td>

        <td style="padding:20px;width:75%;vertical-align:middle">
          MatrixCity: A Large-scale City Dataset for City-scale Neural Rendering and Beyond
          <br>
          
          <strong>Yixuan Li</strong>, <a href="https://jianglh-whu.github.io/">Lihan Jiang</a>, <a href="https://eveneveno.github.io/lnxu/">Linning Xu</a>,<a href="https://kam1107.github.io/">Yuanbo Xiangli</a>, 
          <a href="https://zhenzhiwang.github.io/">Zhenzhi Wang</a>, <a href="http://dahua.site/">Dahua Lin</a>, <a href="https://daibo.info/">Bo Dai</a>.
          <br>
          <em>ICCV</em>, 2023
          <br>
          <a href="https://arxiv.org/abs/2309.16553">arXiv</a> /
          <a href="https://city-super.github.io/matrixcity/">project page</a>
          <p></p>
          <p>A large scale synthetic dataset from Unreal Engine 5 for city-scale NeRF rendering. </p>
        </td>
      </tr>


      <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one"><div class="two" id='dmn_image'>
                    <img src='images/dmn.png' width="160"></div></div>
        </td>

        <td style="padding:20px;width:75%;vertical-align:middle">
          Negative sample matters: A renaissance of metric learning for temporal grounding
          <br>
          
          <strong>Zhenzhi Wang</strong>, <a href="https://wanglimin.github.io/">Limin Wang</a>, Tao Wu, <a href="https://openreview.net/profile?id=~Tianhao_Li1"> Tianhao Li</a>, Gangshan Wu.
          <br>
          <em>AAAI</em>, 2022
          <br>
          <a href="https://arxiv.org/abs/2109.04872">arXiv</a> /
          <a href="https://github.com/MCG-NJU/MMN">code</a>
          <p></p>
          <p>Boost the performance of temporal grounding with contrastive learning by leveraging more negative samples. </p>
        </td>
      </tr>


      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one"><div class="two" id='dmn_image'>
                    <img src='images/multisports.jpg' width="160"></div></div>
        </td>

        <td style="padding:20px;width:75%;vertical-align:middle">
          MultiSports: A Multi-Person Video Dataset of Spatio-Temporally Localized Sports Actions
          <br>
          
          <a href="https://yixuanli98.github.io/">Yixuan Li</a>, Lei Chen, Runyu He, <strong>Zhenzhi Wang</strong>, Gangshan Wu, <a href="https://wanglimin.github.io/">Limin Wang</a>.
          <br>
          <em>ICCV</em>, 2021
          <br>
          <a href="https://arxiv.org/abs/2105.07404">arXiv</a> /
          <a href="https://github.com/MCG-NJU/MultiSports">code</a>
          <p></p>
          <p>A fine-grained and large-scale spatial-temporal action detection dataset with 4 different sports, 66 action categories, 3200 video clips, and annotating 37701 action instances with 902k bounding boxes.</p>
        </td>
      </tr>


      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one"><div class="two" id='dmn_image'>
                    <img src='images/bcn.png' width="160"></div></div>
        </td>

        <td style="padding:20px;width:75%;vertical-align:middle">
          Actions as Moving Points
          <br>
          
          <strong>Zhenzhi Wang</strong>, <a href="https://sebgao.github.io//">Ziteng Gao</a>, <a href="https://wanglimin.github.io/">Limin Wang</a>, Gangshan Wu.
          <br>
          <em>ECCV</em>, 2020
          <br>
          <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123700035.pdf">paper</a> /
          <a href="https://github.com/MCG-NJU/BCN">code</a>
          <p></p>
          <p>We leverage two complementary modules to boost action segmentation performance: (1) stage cascade for boosting segmentation accuracy for hard frames (e.g., near action boundaries); and (2) local barrier pooling utilizing boundary information for smoother predictions and less over-segmentation errors.</p>
        </td>
      </tr> -->

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Professional Services</h2>
                <p>
                  &bull; Conference reviewer for CVPR (2024, <a href="https://cvpr2023.thecvf.com/Conferences/2023/OutstandingReviewers"> 2023</a>, 2022), ICCV (2023), ECCV (2022), WACV (2023). <br>
                  &bull; Journal reviewer for Pattern Recognition, IEEE TNNLS, TCSVT, Neurocomputing.<br>
                  &bull; Co-organizer of <a href="https://deeperaction.github.io/iccv21/index.html">DeeperAction Workshop</a> at ICCV 2021, and ACM-MM 2021 Grand Challenge <a href="https://2021.acmmm.org/multimedia-grand-challenge">Multi-modal ads video understanding</a>.<br>
                </p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Thanks <a href="https://jonbarron.info/">Jon Barron </a> for sharing the source code of this <a href="https://github.com/jonbarron/jonbarron_website">website template</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
